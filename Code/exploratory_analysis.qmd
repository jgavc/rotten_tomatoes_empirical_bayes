---
title: "Exploratory Analysis"
author: "Jack Cunningham"
format: pdf
editor: visual
---

# Reading Data and Loading Libraries

```{r}
library(tidyverse)
library(naniar)
library(ebbr)
```

```{r}
path <- "D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Raw/rotten_tomatoes_movies.csv"
df <- as.data.frame(read_csv(path, show_col_types = FALSE))
head(df)
```

# Selecting Variables of Interest

```{r}
movie_df <- df |> 
  select(rotten_tomatoes_link, movie_title, original_release_date,
         tomatometer_rating, tomatometer_count, audience_rating, audience_count)
head(movie_df)
```

# Looking for missing values

```{r}
colSums(is.na(movie_df))
```

```{r}
vis_miss(movie_df)
```

Since there are so few times where we have missing ratings (and its probably due to very unpopular movies) I'll filter this out.

```{r}
filtered_movie_df <- movie_df |> 
  filter(!if_any(c(audience_rating,audience_count,tomatometer_rating,
                   tomatometer_count), is.na))
colSums(is.na(filtered_movie_df))
```

Now the only missing values we have are for original release date, we haven't decided on how to address dates yet so I'll leave for now.

```{r}
filtered_movie_df |>
  ggplot(aes(x = tomatometer_rating)) +
  geom_histogram()
```

Lets see how this changes if we introduce a filter on the amount of tomato meter count.

```{r}
quantile(filtered_movie_df$tomatometer_count)
```

So lets do 12 for now.

```{r}
filtered_movie_df |>
  filter(tomatometer_count > 12) |> 
  ggplot(aes(x = tomatometer_rating)) +
  geom_histogram()
```

it seems this distribution trends up and peaks around rating 87.5. Lets also do a density plot.

```{r}
filtered_movie_df |>
  filter(tomatometer_count > 30) |> 
  ggplot(aes(x = tomatometer_rating)) +
  geom_density()
```

Before we fit a distribution we need to construct a "fresh critics count", aka the number of critics who gave this movie a positive review.

```{r}
filtered_movie_df <- filtered_movie_df |> 
  mutate(tomato_meter_fresh_critics_count = round((1/100)*tomatometer_rating*
            tomatometer_count),
         audience_rating_fresh_count = round((1/100) * audience_rating *
                                               audience_count))
```

```{r}
prior <- filtered_movie_df |>
  filter(tomatometer_count > 30) |> 
  ebb_fit_prior(tomato_meter_fresh_critics_count, tomatometer_count)
prior
```

```{r}
prior2 <- filtered_movie_df |>
  filter(audience_count > 1000) |> 
  ebb_fit_prior(audience_rating_fresh_count, audience_count)
prior2
```

```{r}
filtered_movie_df |> 
  filter(tomatometer_count > 30) |> 
ggplot(aes(x = tomatometer_rating/100)) +
  geom_histogram(aes(y = after_stat(density)), bins = 50) +
  stat_function(
    fun = dbeta,
    args = list(shape1 = prior$parameters[[1]], shape2 = prior$parameters[[2]]),
    linewidth = 1
  ) +
  labs(title = "Histogram with Density Overlay")
```

This seems to fit not too great, it seems there might be a mixture situation here. For now lets stick with it.

```{r}
eb_movies <- filtered_movie_df |> 
  add_ebb_estimate(tomato_meter_fresh_critics_count,tomatometer_count,
                   prior_subset = tomatometer_count > 30)
eb_movies <- as_tibble(eb_movies)
head(eb_movies)
```

```{r}
eb_movies |> 
  slice_max(.raw, n = 10) |> 
  select(tomatometer_count,.raw,.fitted)
```

```{r}
eb_movies |> 
  ggplot(aes(.raw, .fitted, color = tomatometer_count)) +
  geom_point() +
  geom_abline(color = "red") +
  #geom_hline(yintercept = tidy(prior)$mean, color = "red") +
  scale_color_continuous(trans = "log",breaks = c(1,10,50,400)) +
  labs(x = "Raw Movie Score",
       y = "Shrunked Movie Score")
  #xlim(c(.8,1)) +
  #ylim(c(.8,1))
```

```{r}
eb_movies |> 
  slice_max(.fitted, n = 25)
```

# Mixture Distribution

Since it seems that our fit is not fantastic, lets try a mixed distribution.

```{r}
library(BMix)

k <- filtered_movie_df$tomato_meter_fresh_critics_count
n <- filtered_movie_df$tomatometer_count

df <- data.frame(k,n)
fit <- bmixfit(df)

```

We see that there are two betas, one with a mean of .3422 and another with .8183. Lets take a look at the parameters.

```{r}
Parameters(fit)
```

```{r}
# --- Beta-Binomial log pmf (stable) ---
log_beta_binom_pmf <- function(k, n, a, b) {
  # log[ choose(n,k) * Beta(k+a, n-k+b) / Beta(a,b) ]
  lchoose(n, k) +
    lbeta(k + a, n - k + b) -
    lbeta(a, b)
}

# --- Weighted negative log-likelihood for one component ---
neg_wll_component <- function(par_log, k, n, w) {
  a <- exp(par_log[1])
  b <- exp(par_log[2])
  # weighted log-likelihood
  ll <- sum(w * log_beta_binom_pmf(k, n, a, b))
  -ll
}

# --- Helper: initialize alpha/beta from mean/var of p = k/n ---
mom_beta_ab <- function(p, w = NULL) {
  if (is.null(w)) w <- rep(1, length(p))
  w <- w / sum(w)
  m <- sum(w * p)
  v <- sum(w * (p - m)^2)

  # If variance is tiny / degenerate, set large concentration
  if (!is.finite(v) || v < 1e-6) {
    phi <- 200
    return(c(alpha = max(1e-3, m * phi),
             beta  = max(1e-3, (1 - m) * phi)))
  }

  # Method of moments: v = m(1-m)/(phi+1)
  phi <- m * (1 - m) / v - 1
  if (!is.finite(phi) || phi <= 0) phi <- 10

  c(alpha = max(1e-3, m * phi),
    beta  = max(1e-3, (1 - m) * phi))
}

# --- Main EM routine ---
fit_beta_binom_mixture <- function(k, n, K = 2, max_iter = 200, tol = 1e-6,
                                   verbose = TRUE, seed = 1) {
  stopifnot(K == 2)
  k <- as.integer(k)
  n <- as.integer(n)

  ok <- is.finite(k) & is.finite(n) & n > 0 & k >= 0 & k <= n
  k <- k[ok]; n <- n[ok]
  p <- k / n

  set.seed(seed)
  # init: kmeans on proportions
  km <- kmeans(p, centers = 2)
  z <- km$cluster

  ab1 <- mom_beta_ab(p[z == 1])
  ab2 <- mom_beta_ab(p[z == 2])
  pi1 <- mean(z == 1)
  pi2 <- 1 - pi1

  a1 <- ab1["alpha"]; b1 <- ab1["beta"]
  a2 <- ab2["alpha"]; b2 <- ab2["beta"]

  loglik_prev <- -Inf

  for (iter in 1:max_iter) {

    # E-step: responsibilities
    ll1 <- log(pi1) + log_beta_binom_pmf(k, n, a1, b1)
    ll2 <- log(pi2) + log_beta_binom_pmf(k, n, a2, b2)

    # log-sum-exp
    mll <- pmax(ll1, ll2)
    denom <- mll + log(exp(ll1 - mll) + exp(ll2 - mll))

    r1 <- exp(ll1 - denom)  # responsibility for component 1
    r2 <- 1 - r1

    loglik <- sum(denom)

    # check convergence
    if (verbose && (iter %% 5 == 0 || iter == 1)) {
      cat(sprintf("iter %3d | logLik %.3f | pi1 %.3f | (a1,b1)=(%.2f,%.2f) | (a2,b2)=(%.2f,%.2f)\n",
                  iter, loglik, pi1, a1, b1, a2, b2))
    }

    if (abs(loglik - loglik_prev) < tol) break
    loglik_prev <- loglik

    # M-step: update mixing weights
    pi1 <- mean(r1)
    pi2 <- 1 - pi1

    # M-step: update (a,b) for each component via weighted MLE
    opt1 <- optim(par = log(c(a1, b1)),
                  fn = neg_wll_component,
                  k = k, n = n, w = r1,
                  method = "BFGS")
    a1 <- exp(opt1$par[1]); b1 <- exp(opt1$par[2])

    opt2 <- optim(par = log(c(a2, b2)),
                  fn = neg_wll_component,
                  k = k, n = n, w = r2,
                  method = "BFGS")
    a2 <- exp(opt2$par[1]); b2 <- exp(opt2$par[2])

    # guardrails
    a1 <- max(a1, 1e-6); b1 <- max(b1, 1e-6)
    a2 <- max(a2, 1e-6); b2 <- max(b2, 1e-6)
    pi1 <- min(max(pi1, 1e-6), 1 - 1e-6)
    pi2 <- 1 - pi1
  }

  list(
    K = 2,
    pi = c(pi1, pi2),
    alpha = c(a1, a2),
    beta  = c(b1, b2),
    logLik = loglik_prev,
    responsibilities = cbind(r1 = r1, r2 = r2),
    used_rows = which(ok),
    iterations = iter
  )
}

```

```{r,cache=TRUE}
mm <- fit_beta_binom_mixture(
  k = filtered_movie_df$tomato_meter_fresh_critics_count,
  n = filtered_movie_df$tomatometer_count,
  K = 2,
  verbose = TRUE
)

mm$pi
mm$alpha
mm$beta
```

```{r}
# Mixture beta density for underlying p
mix_beta_density <- function(x, pi, alpha, beta) {
  pi[1] * dbeta(x, alpha[1], beta[1]) + pi[2] * dbeta(x, alpha[2], beta[2])
}

reduced_movies <- filtered_movie_df |> filter(tomatometer_count > 50)
p <- reduced_movies$tomato_meter_fresh_critics_count / reduced_movies$tomatometer_count
p <- p[is.finite(p)]

hist(p, breaks = 40, freq = FALSE, main = "Proportions with Beta Mixture Overlay", xlab = "fresh / total")
curve(mix_beta_density(x, mm$pi, mm$alpha, mm$beta), add = TRUE, lwd = 2)

```

This looks better to me.

```{r}
# movie-level posterior parameters for a K-component beta-mixture prior
posterior_params_mixture <- function(y, n, pi, alpha, beta) {
  K <- length(pi)

  # log unnormalized weights: log pi_k + log Beta-Binomial marginal
  ll <- log(pi) +
    lchoose(n, y) +
    lbeta(y + alpha, n - y + beta) -
    lbeta(alpha, beta)

  # stabilize and normalize
  w <- exp(ll - max(ll))
  w <- w / sum(w)

  # posterior component parameters
  post_alpha <- alpha + y
  post_beta  <- beta + (n - y)

  list(w = w, post_alpha = post_alpha, post_beta = post_beta)
}

```

```{r}
K <- length(mm$pi)
stopifnot(K == 2)

out <- filtered_movie_df |>
  dplyr::mutate(
    post = purrr::pmap(
      list(tomato_meter_fresh_critics_count, tomatometer_count),
      ~ posterior_params_mixture(..1, ..2, mm$pi, mm$alpha, mm$beta)
    ),
    w1 = purrr::map_dbl(post, ~ .x$w[1]),
    w2 = purrr::map_dbl(post, ~ .x$w[2]),
    a1 = purrr::map_dbl(post, ~ .x$post_alpha[1]),
    b1 = purrr::map_dbl(post, ~ .x$post_beta[1]),
    a2 = purrr::map_dbl(post, ~ .x$post_alpha[2]),
    b2 = purrr::map_dbl(post, ~ .x$post_beta[2])
  ) |>
  dplyr::select(-post)

```

```{r}
sample_from_stored_posterior <- function(w1, a1, b1, a2, b2, S = 2000) {
  z <- rbinom(S, size = 1, prob = w1) # 1 => component 1
  # if z==1 sample from (a1,b1), else (a2,b2)
  out <- numeric(S)
  idx1 <- which(z == 1)
  idx2 <- which(z == 0)
  if (length(idx1)) out[idx1] <- rbeta(length(idx1), a1, b1)
  if (length(idx2)) out[idx2] <- rbeta(length(idx2), a2, b2)
  out
}
```

```{r}
set.seed(1)
out2 <- out |>
  dplyr::mutate(
    post_mean = w1 * (a1 / (a1 + b1)) + w2 * (a2 / (a2 + b2)),
    ci = purrr::pmap(
      list(w1, a1, b1, a2, b2),
      ~ {
        d <- sample_from_stored_posterior(..1, ..2, ..3, ..4, ..5, S = 800)
        as.numeric(stats::quantile(d, c(.025, .975), names = FALSE))
      }
    ),
    lo_95 = purrr::map_dbl(ci, 1),
    hi_95 = purrr::map_dbl(ci, 2)
  ) |>
  dplyr::select(-ci)
```

```{r}
posterior_tbl <- out2 |>
  dplyr::select(rotten_tomatoes_link, movie_title,
                tomatometer_count, tomato_meter_fresh_critics_count,
                w1, w2, a1, b1, a2, b2,
                post_mean, lo_95, hi_95)
```

# Saving results

```{r}
save(posterior_tbl, file = "D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/posterior_tbl.RData")
```
