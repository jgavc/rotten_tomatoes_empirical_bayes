---
title: "How can movie reviews be better understood? (working title)"
author: "Jack Cunningham"
format: html
editor: visual
---

```{r, loading data}
modeling_data <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/mixed_beta_output.RData")
posterior_stats <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/posterior_stats.RData")
load("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/movie_df.RData")
mixed_beta_output <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/mixed_beta_output.RData")
```

```{r, loading functions}
helper_functions <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Functions/fit_mixed_beta_helpers.RData")
```

```{r}
mix_beta_density <- helper_functions$mix_beta_density
```

```{r, loading libraries}
library(tidyverse)
```

# Research Motivation & Question

Lets say you want to watch a movie at home, how should you decide on what movie is best?

An idea would be checking a movie review site to see what is reviewed best. Rotten Tomatoes provides the percentage of critic and audience reviews that enjoyed a movie enough to give it a "fresh" rating. What movies have the highest tomatometer?

![](images/top_movies.JPG)

Hmm, I've never heard of these movies before. Let's take a look at 'Twas the Fight Before Christmas. Maybe its the next Christmas Story.

![](images/twas_the_fight.JPG)

Immediately we see an issue with relying solely on the tomatometer, it can be a perfect 100% if only a few critics have given favorable reviews.

How can we create a metric that lets us find and compare movies in a fair way? Empirical Bayes provides a solution!

# Data Overview

The data for this analysis has been kindly scraped by Stefano Leone who has shared this data on [Kaggle](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset). It provides us with data from Rotten Tomatoes for 17,000 movies up to the end of 2020. For each movie we have the number of critics and audience who reviewed a movie and the percentage who reviewed them as fresh, exactly what we need to perform this analysis.

# Constructing a Prior

Empirical Bayes approaches are very helpful for situations where the statistic of interest is a proportion and our goal is to compare the "true" proportions after adjusting for known information about movie reviews. Since we want to choose the best movie, and tomatometer is a percentage of reviews that are positive, and we have a large number of movies (17,000) this method is perfect

The first step is to take a look at the distribution of tomatometer for movies with a certain breakpoint of critic reviews.

```{r, filtered movies}
reduced_movies <- movie_df |> 
  mutate(p = tomato_meter_fresh_critics_count / tomatometer_count) |> 
  filter(tomatometer_count > 15)
```

```{r, fig 1}
reduced_movies |> 
  ggplot(aes(x = tomatometer_rating/100)) +
  geom_histogram(bins = 20,color = "white", fill = "#2C3E50") +
  theme_minimal() +
  labs(x = "Tomatometer", y = "Count", title = "Distribution of Tomatometer",
       subtitle = "Movies with 15 or greater reviews")

```

There are very few movies with a tomatometer near zero, almost an equal amount distributed between 10% to 50%, then a large peak around 85%, tailing off near a perfect score of 100%.

We fit a distribution to this observed data to form an empirical prior, in this case a mixed beta distribution will fit nicely.

```{r}
reduced_movies |> 
  ggplot(aes(x = tomatometer_rating/100)) +
  geom_histogram(aes(y = after_stat(density)),bins = 20,color = "white", fill = "#2C3E50") +
  stat_function(fun = mix_beta_density, 
              args = list(pi =  modeling_data$mm_critic$pi, alpha = modeling_data$mm_critic$alpha, beta = modeling_data$mm_critic$beta),
              color = "red") +
  labs(x = "Tomatometer", y = "", title = "Empirical Prior",
       subtitle = "Mixed Beta fit") +
  theme_minimal()
```

Now with this prior we can analyze our posterior estimates of tomatometer for each movie.

# Posterior Analysis; How have our estimates changed?

One of the most helpful aspects of emprical bayes is how it "shrinks" estimates. This shrinking occurs strongly for movies with only a few reviews, and lightly for movies with many reviews (as the observed data dominates the information provided by the prior). Let's see how shrinking affects the posterior estimates as the count of reviews change.

Let's take a look at what happens to the movies with the highest tomatometer rating as the number of reviews change.

```{r}
perfect_scores <- critic_posterior_stats |> 
  filter(tomatometer_rating == 100) 
perfect_scores |> 
  ggplot(aes(x = tomatometer_count, y = post_mean)) +
  geom_point()
```

```{r}
critic_posterior_stats <- posterior_stats$critic_posterior_stats
critic_posterior_stats |> 
  slice_max(tomatometer_rating, n = 100)
```

```{r}
reduced_movies <- movie_df |> filter(tomatometer_count > 50)
p <- reduced_movies$tomato_meter_fresh_critics_count / reduced_movies$tomatometer_count
p <- p[is.finite(p)]

hist(p, breaks = 40, freq = FALSE, main = "Proportions with Beta Mixture Overlay", xlab = "fresh / total")
curve(mix_beta_density(x, modeling_data$mm_critic$pi, modeling_data$mm_critic$alpha, modeling_data$mm_critic$beta), add = TRUE, lwd = 2)
```
