---
title: "How can movie reviews be better understood? (working title)"
author: "Jack Cunningham"
format: html
editor: visual
---

```{r, loading data}
modeling_data <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/mixed_beta_output.RData")
posterior_stats <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/posterior_stats.RData")
load("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/movie_df.RData")
mixed_beta_output <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Processed/mixed_beta_output.RData")
```

```{r, loading functions}
helper_functions <- readRDS("D:/Portfolio Projects/rotten_tomatoes_empirical_bayes/Data/Functions/fit_mixed_beta_helpers.RData")
```

```{r}
mix_beta_density <- helper_functions$mix_beta_density
```

```{r, loading libraries}
library(tidyverse)
```

# Research Motivation & Question

Lets say you want to watch a movie at home, how should you decide on what movie is best?

An idea would be checking a movie review site to see what is reviewed best. Rotten Tomatoes provides the percentage of critic and audience reviews that enjoyed a movie enough to give it a "fresh" rating. What movies have the highest tomatometer?

![](images/top_movies.JPG)

Hmm, I've never heard of these movies before. Let's take a look at 'Twas the Fight Before Christmas. Maybe its the next Christmas Story.

![](images/twas_the_fight.JPG)

Immediately we see an issue with relying solely on the tomatometer, it can be a perfect 100% if only a few critics have given favorable reviews.

How can we create a metric that lets us find and compare movies in a fair way? Empirical Bayes provides a solution!

# Data Overview

The data for this analysis has been kindly scraped by Stefano Leone who has shared this data on [Kaggle](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset). It provides us with data from Rotten Tomatoes for 17,000 movies up to the end of 2020. For each movie we have the number of critics and audience who reviewed a movie and the percentage who reviewed them as fresh, exactly what we need to perform this analysis.

# Constructing a Prior

Empirical Bayes approaches are very helpful for situations where the statistic of interest is a proportion and our goal is to compare the "true" proportions after adjusting for known information about movie reviews. Since we want to choose the best movie, and tomatometer is a percentage of reviews that are positive, and we have a large number of movies (17,000) this method is perfect

The first step is to take a look at the distribution of tomatometer for movies with a certain breakpoint of critic reviews.

```{r, filtered movies}
reduced_movies <- movie_df |> 
  mutate(p = tomato_meter_fresh_critics_count / tomatometer_count) |> 
  filter(tomatometer_count > 15)
```

```{r, fig 1}
reduced_movies |> 
  ggplot(aes(x = tomatometer_rating/100)) +
  geom_histogram(bins = 20,color = "white", fill = "#2C3E50") +
  theme_minimal() +
  labs(x = "Tomatometer", y = "Count", title = "Distribution of Tomatometer",
       subtitle = "Movies with 15 or greater reviews")

```

There are very few movies with a tomatometer near zero, almost an equal amount distributed between 10% to 50%, then a large peak around 85%, tailing off near a perfect score of 100%.

We fit a distribution to this observed data to form an empirical prior, in this case a mixed beta distribution will fit nicely.

```{r}
reduced_movies |> 
  ggplot(aes(x = tomatometer_rating/100)) +
  geom_histogram(aes(y = after_stat(density)),bins = 20,color = "white", fill = "#2C3E50") +
  stat_function(fun = mix_beta_density, 
              args = list(pi =  modeling_data$mm_critic$pi, alpha = modeling_data$mm_critic$alpha, beta = modeling_data$mm_critic$beta),
              color = "red") +
  labs(x = "Tomatometer", y = "", title = "Empirical Prior",
       subtitle = "Mixed Beta fit") +
  theme_minimal()
```

Now with this prior we can analyze our posterior estimates of tomatometer for each movie.

# Posterior Analysis; How have our estimates changed?

One of the most helpful aspects of emprical bayes is how it "shrinks" estimates. This shrinking occurs strongly for movies with only a few reviews, and lightly for movies with many reviews (as the observed data dominates the information provided by the prior). Let's see how shrinking affects the posterior estimates as the count of reviews change.

Let's take a look at what this looks like:

```{r}
shrinkage_df <- critic_posterior_stats |> 
  filter(tomatometer_rating < 15 | tomatometer_rating > 85) |> 
  mutate(high_scoring = ifelse(tomatometer_rating > 85, "High Score",
                               "Low Score")) |> 
  mutate(high_scoring = factor(high_scoring, c("Low Score","High Score")))
```

```{r}
shrinkage_df |> 
  ggplot(aes(x = tomatometer_rating, y = post_mean*100, col = log(tomatometer_count))) +
  geom_point() +
  geom_abline(col = "#26F7FD") +
  facet_wrap(~ high_scoring, scales = "free") +
  theme_minimal() +
  labs(x = "Observed Tomatometer Rating", y = "Adjusted Tomatometer Rating",
       title = "How Bayesian Shrinking Affects Extreme Observed Ratings",
       subtitle = "Line is the observed tomatometer") +
  scale_color_gradient(low = "#08306b", high = "#26F7FD",
                       name = "Critic Reviews",
                        breaks = log(c(10,100,500)),
                        labels = c("10","100","500")) +
  guides(color = guide_colorbar(
  barheight = 8,
  barwidth = 1
))
```

The shrinking has its greatest effect on extreme values with only a few reviews.

For instance, for an observed tomatometer rating of zero having just one critic review driving that score brings the adjusted rating up to 20. This makes sense, a single negative review does not communicate much about the quality of a movie and there are not many observed zeros in movies with greater than 15 reviews in our data.

On the other hand, for a perfect observed tomatometer rating of one hundred we see that the prior adjusts the tomatometer down. Once again, since there are few observed perfect scores for movies with greater than 15 reviews the estimate is driven mainly by the prior until many reviews are recorded.

This shrinkage is a huge help for finding the most beloved movies by critics.

```{r}
critic_posterior_stats |> 
  slice_max(post_mean, n = 10) |>
  rename()
```

```{r}
critic_posterior_stats |> 
  ggplot(aes(x = tomatometer_rating, y = post_mean, 
             col = log(tomatometer_count))) +
  geom_point() +
  xlim(0,25) +
  ylim(0,.35)
  
```

```{r}
reduced_movies <- movie_df |> filter(tomatometer_count > 50)
p <- reduced_movies$tomato_meter_fresh_critics_count / reduced_movies$tomatometer_count
p <- p[is.finite(p)]

hist(p, breaks = 40, freq = FALSE, main = "Proportions with Beta Mixture Overlay", xlab = "fresh / total")
curve(mix_beta_density(x, modeling_data$mm_critic$pi, modeling_data$mm_critic$alpha, modeling_data$mm_critic$beta), add = TRUE, lwd = 2)
```
